<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Attention Models and Transformers | NLP Galore</title>
<meta name="author" content="Various Sources">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Chapter 8 Attention Models and Transformers | NLP Galore">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Attention Models and Transformers | NLP Galore">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="https://www.apronus.com/math/transformer-language-model-definition  8.1 Attention  8.1.1 Basic Idea In the context of language translation, the basic concept of attention is that each time the...">
<meta property="og:description" content="https://www.apronus.com/math/transformer-language-model-definition  8.1 Attention  8.1.1 Basic Idea In the context of language translation, the basic concept of attention is that each time the...">
<meta name="twitter:description" content="https://www.apronus.com/math/transformer-language-model-definition  8.1 Attention  8.1.1 Basic Idea In the context of language translation, the basic concept of attention is that each time the...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">NLP Galore</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">BASICS OF NLP</li>
<li><a class="" href="dictionary-based-methods.html"><span class="header-section-number">2</span> Dictionary-Based Methods</a></li>
<li><a class="" href="regression-based-methods.html"><span class="header-section-number">3</span> Regression-Based Methods</a></li>
<li><a class="" href="bag-of-words-tf-idf.html"><span class="header-section-number">4</span> Bag of Words: TF-IDF</a></li>
<li><a class="" href="basic-word-embeddings-word2vec-glove.html"><span class="header-section-number">5</span> Basic Word Embeddings: Word2Vec &amp; Glove</a></li>
<li><a class="" href="topic-modelling-lda.html"><span class="header-section-number">6</span> Topic Modelling: LDA</a></li>
<li><a class="" href="sequence-models-rnns-and-lstms.html"><span class="header-section-number">7</span> Sequence Models: RNNs and LSTMs</a></li>
<li><a class="active" href="attention-models-and-transformers.html"><span class="header-section-number">8</span> Attention Models and Transformers</a></li>
<li><a class="" href="pretrained-models-and-fine-tuning.html"><span class="header-section-number">9</span> Pretrained Models and Fine-Tuning</a></li>
<li><a class="" href="other-useful-methods-for-textual-analysis.html"><span class="header-section-number">10</span> Other Useful Methods for Textual Analysis</a></li>
<li class="book-part">APPLICATIONS IN ECON/FINANCE</li>
<li><a class="" href="sentiment-analysis.html"><span class="header-section-number">11</span> Sentiment Analysis</a></li>
<li class="book-part">CODE SNIPPETS</li>
<li><a class="" href="data-scraping.html"><span class="header-section-number">12</span> Data Scraping</a></li>
<li><a class="" href="data-cleaning.html"><span class="header-section-number">13</span> Data Cleaning</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="attention-models-and-transformers" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Attention Models and Transformers<a class="anchor" aria-label="anchor" href="#attention-models-and-transformers"><i class="fas fa-link"></i></a>
</h1>
<p><a href="https://www.apronus.com/math/transformer-language-model-definition" class="uri">https://www.apronus.com/math/transformer-language-model-definition</a></p>
<div id="attention" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Attention<a class="anchor" aria-label="anchor" href="#attention"><i class="fas fa-link"></i></a>
</h2>
<div id="basic-idea" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Basic Idea<a class="anchor" aria-label="anchor" href="#basic-idea"><i class="fas fa-link"></i></a>
</h3>
<p>In the context of language translation, the basic concept of attention is that each time the model predicts an output word, it only pays attention to some input words. Specifically, for each word in the output sentence, it will map the important and relevant words from the input sentence and assign higher weights to these words as shown in the figure below:</p>
<div class="figure">
<img src="Figures/transformers_01.png" alt=""><p class="caption">image-20220816201407808</p>
</div>
<p>In the traditional Seq2Seq model, we discard all the intermediate states of the encoder and use only its final states (vector) to initialize the decoder. The central idea behind attention is not to throw away these intermediate encoder states but to utilize all the states.</p>
</div>
<div id="steps-for-computing-attention" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Steps for Computing Attention<a class="anchor" aria-label="anchor" href="#steps-for-computing-attention"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="family-of-attention-mechanisms" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Family of Attention Mechanisms<a class="anchor" aria-label="anchor" href="#family-of-attention-mechanisms"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>There are different types of attention mechanisms: <a href="https://lilianweng.github.io/posts/2018-06-24-attention/" class="uri">https://lilianweng.github.io/posts/2018-06-24-attention/</a>
</li>
</ul>
</div>
<div id="self-attention" class="section level3" number="8.1.4">
<h3>
<span class="header-section-number">8.1.4</span> Self-Attention<a class="anchor" aria-label="anchor" href="#self-attention"><i class="fas fa-link"></i></a>
</h3>
<p>Self-attention is equivalent to a generalized attention mechanism where the query, key, and values take the same input.</p>
</div>
</div>
<div id="transformers" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Transformers<a class="anchor" aria-label="anchor" href="#transformers"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><a href="https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/McmHduRWJynsjZjx5" class="uri">https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/McmHduRWJynsjZjx5</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></li>
</ul>
<div id="overview-1" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Overview<a class="anchor" aria-label="anchor" href="#overview-1"><i class="fas fa-link"></i></a>
</h3>
<p>In its simplest form, a transformer takes in a sentence to translate and outputs a translated sentence:
<span class="math display">\[
input \to transformer \to output
\]</span>
The transformer consists of an encoder and a decoder:
<span class="math display">\[
input \to encoder \to decoder \to output
\]</span>
The encoder and the decoder consist of layers:</p>
<div class="figure">
<img src="Figures/transformers_02.png" alt=""><p class="caption">image-20220816203617350</p>
</div>
<ul>
<li>Notice that the sublayers are stacked linearly for both the encoder and the decoder, but the decoder takes an additional input: the output from the last encoder sub-layer.</li>
</ul>
<p>Finally, each layer of the encoder and decoder take the following structure:</p>
<div class="figure">
<img src="Figures/transformers_03.png" alt=""><p class="caption">image-20220816204337867</p>
</div>
<ul>
<li>The encoder layer computes the self-attention and then feeds it into the feed-forward network.</li>
<li>The decoder layer is identical to the encoder layer with additional middle step: the encoder-decoder attention, which uses the information carried over from the last step of the encoder.</li>
</ul>
</div>
<div id="step-1.-embedding" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> Step 1. Embedding<a class="anchor" aria-label="anchor" href="#step-1.-embedding"><i class="fas fa-link"></i></a>
</h3>
<p>Before the input sentence can be fed into the architecture, it needs to be embedded via (1) word embedding and (2) positional encoding. Obviously, this embedding only happens in the bottom-most encoder.</p>
<ul>
<li>The word embedding is typically done through a pre-trained neural network instead of a simple one-hot encoding.</li>
</ul>
<p>Positional encoding is also necessary because the transformer architecture does not include a default method for analyzing the order of words in the input sentence. In transformers, each position (index) is mapped to a vector, and hence the output of the positional encoding layer is a matrix where the <span class="math inline">\(i\)</span>th row is a vector representing the <span class="math inline">\(i\)</span>th token in the input sequence. This vector is then added to the original word embedding and fed into the very first encoder layer.</p>
<ul>
<li>See <a href="https://jalammar.github.io/illustrated-transformer/">this post</a> for details on the construction of this matrix.</li>
</ul>
</div>
<div id="step-2.-self-attention-in-encoders" class="section level3" number="8.2.3">
<h3>
<span class="header-section-number">8.2.3</span> Step 2. Self-Attention in Encoders<a class="anchor" aria-label="anchor" href="#step-2.-self-attention-in-encoders"><i class="fas fa-link"></i></a>
</h3>
<p>Self-attention is a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. For example, suppose we are given the following input sentence: “Simon called Manav because he felt overwhelmed.” Self-attention allows the model to associate “he” with “Simon” and not “Manav.”</p>
<p>For each word embedding vector, there is a vector of the attention layer. But we require the entire input sequence (i.e. the list of words) for computing self-attention because (1) we need it to compute the weights (= normalized scores) and (2) we need to combine the weights with the value vectors, each of which is associated with each word input. <a href="https://jalammar.github.io/illustrated-transformer/">This post</a> describes the step-by-step process for computing self-attention very nicely, which is summarized below:</p>
<div class="figure">
<img src="Figures/transformers_04.png" alt=""><p class="caption">image-20220816214440095</p>
</div>
<p><strong>Step 1. Create Query, Key, and Value vectors</strong></p>
<p>The first step is to create, for each word, a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that trained during the training process.</p>
<ul>
<li>Note that typically these new vectors are smaller in dimension than the embedding vector.</li>
</ul>
<p><strong>Step 2. Compute the Score</strong></p>
<p>For each word, the score is is calculated by taking the dot product of the query vector with the key vector of the respective word. So for the word in position <span class="math inline">\(i\)</span>, the first score would be the dot product of <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_1\)</span> and the second score would be the dot product of <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_2\)</span>.</p>
<p><strong>Step 3. Convert the Scores to Weights</strong></p>
<p>The scores are divided by <span class="math inline">\(\sqrt{d}\)</span> where <span class="math inline">\(d\)</span> is the dimension of the key vectors, which is intended to lead to more stable gradients. Then they are passed through a softmax operation so that the scores are converted to weights. The resulting softmax score determines how much each word will be expressed at this position.</p>
<p><strong>Step 4. Construct the self attention layer</strong></p>
<p>Each value vector is multiplied by the softmax score and then the weighted value vectors are summed to produce the output of the self-attention layer.</p>
<p><strong>Note: Multi-headed Attention</strong></p>
</div>
<div id="step-3.-self-attention-in-decoders" class="section level3" number="8.2.4">
<h3>
<span class="header-section-number">8.2.4</span> Step 3. Self-Attention in Decoders<a class="anchor" aria-label="anchor" href="#step-3.-self-attention-in-decoders"><i class="fas fa-link"></i></a>
</h3>
<p>Similarly as before, the decoder passes its input into a multi-head self-attention layer. Unlike the one in the encoder, it is only allowed to attend to earlier positions in the sequence. This is done by masking future positions.</p>
</div>
<div id="step-4.-encoder-decoder-attention-in-decoders" class="section level3" number="8.2.5">
<h3>
<span class="header-section-number">8.2.5</span> Step 4. Encoder-Decoder Attention in Decoders<a class="anchor" aria-label="anchor" href="#step-4.-encoder-decoder-attention-in-decoders"><i class="fas fa-link"></i></a>
</h3>
<p>This additional layer works like self-attention except that it combines two sources of inputs: the self-attention layer below it as well as the output of the encoder stack. <strong>Importantly, the output from the encoder stack is passed to the value and key parameters, while the output of the self-attention module is passed to the query parameter.</strong></p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sequence-models-rnns-and-lstms.html"><span class="header-section-number">7</span> Sequence Models: RNNs and LSTMs</a></div>
<div class="next"><a href="pretrained-models-and-fine-tuning.html"><span class="header-section-number">9</span> Pretrained Models and Fine-Tuning</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#attention-models-and-transformers"><span class="header-section-number">8</span> Attention Models and Transformers</a></li>
<li>
<a class="nav-link" href="#attention"><span class="header-section-number">8.1</span> Attention</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-idea"><span class="header-section-number">8.1.1</span> Basic Idea</a></li>
<li><a class="nav-link" href="#steps-for-computing-attention"><span class="header-section-number">8.1.2</span> Steps for Computing Attention</a></li>
<li><a class="nav-link" href="#family-of-attention-mechanisms"><span class="header-section-number">8.1.3</span> Family of Attention Mechanisms</a></li>
<li><a class="nav-link" href="#self-attention"><span class="header-section-number">8.1.4</span> Self-Attention</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#transformers"><span class="header-section-number">8.2</span> Transformers</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-1"><span class="header-section-number">8.2.1</span> Overview</a></li>
<li><a class="nav-link" href="#step-1.-embedding"><span class="header-section-number">8.2.2</span> Step 1. Embedding</a></li>
<li><a class="nav-link" href="#step-2.-self-attention-in-encoders"><span class="header-section-number">8.2.3</span> Step 2. Self-Attention in Encoders</a></li>
<li><a class="nav-link" href="#step-3.-self-attention-in-decoders"><span class="header-section-number">8.2.4</span> Step 3. Self-Attention in Decoders</a></li>
<li><a class="nav-link" href="#step-4.-encoder-decoder-attention-in-decoders"><span class="header-section-number">8.2.5</span> Step 4. Encoder-Decoder Attention in Decoders</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>NLP Galore</strong>" was written by Various Sources. It was last built on 2022-08-29.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
