<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Attention Models and Transformers | NLP Galore</title>
<meta name="author" content="Various Sources">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Chapter 8 Attention Models and Transformers | NLP Galore">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Attention Models and Transformers | NLP Galore">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<meta name="description" content="Transformers are one particular form of the encoder-decoder architecture that uses only attention and does not use RNNs. Main applications are BERT (Bidirectional Encoder Representations from...">
<meta property="og:description" content="Transformers are one particular form of the encoder-decoder architecture that uses only attention and does not use RNNs. Main applications are BERT (Bidirectional Encoder Representations from...">
<meta name="twitter:description" content="Transformers are one particular form of the encoder-decoder architecture that uses only attention and does not use RNNs. Main applications are BERT (Bidirectional Encoder Representations from...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">NLP Galore</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">BASICS OF NLP</li>
<li><a class="" href="dictionary-based-methods.html"><span class="header-section-number">2</span> Dictionary-Based Methods</a></li>
<li><a class="" href="regression-based-methods.html"><span class="header-section-number">3</span> Regression-Based Methods</a></li>
<li><a class="" href="bag-of-words-tf-idf.html"><span class="header-section-number">4</span> Bag of Words: TF-IDF</a></li>
<li><a class="" href="basic-word-embeddings-word2vec-glove.html"><span class="header-section-number">5</span> Basic Word Embeddings: Word2Vec &amp; Glove</a></li>
<li><a class="" href="topic-modelling-lda.html"><span class="header-section-number">6</span> Topic Modelling: LDA</a></li>
<li><a class="" href="sequence-models-rnns-and-lstms.html"><span class="header-section-number">7</span> Sequence Models: RNNs and LSTMs</a></li>
<li><a class="active" href="attention-models-and-transformers.html"><span class="header-section-number">8</span> Attention Models and Transformers</a></li>
<li><a class="" href="pretrained-models-and-fine-tuning.html"><span class="header-section-number">9</span> Pretrained Models and Fine-Tuning</a></li>
<li><a class="" href="other-useful-methods-for-textual-analysis.html"><span class="header-section-number">10</span> Other Useful Methods for Textual Analysis</a></li>
<li class="book-part">APPLICATIONS IN ECON/FINANCE</li>
<li><a class="" href="sentiment-analysis.html"><span class="header-section-number">11</span> Sentiment Analysis</a></li>
<li class="book-part">CODE SNIPPETS</li>
<li><a class="" href="data-scraping.html"><span class="header-section-number">12</span> Data Scraping</a></li>
<li><a class="" href="data-cleaning.html"><span class="header-section-number">13</span> Data Cleaning</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="attention-models-and-transformers" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Attention Models and Transformers<a class="anchor" aria-label="anchor" href="#attention-models-and-transformers"><i class="fas fa-link"></i></a>
</h1>
<p>Transformers are one particular form of the encoder-decoder architecture that uses only attention and does not use RNNs. Main applications are BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-Trained Transformer). In this section, we first describe the attention mechanism and then the transformer architecture.</p>
<div id="attention" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Attention<a class="anchor" aria-label="anchor" href="#attention"><i class="fas fa-link"></i></a>
</h2>
<div id="basic-idea" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Basic Idea<a class="anchor" aria-label="anchor" href="#basic-idea"><i class="fas fa-link"></i></a>
</h3>
<p>In the context of language translation, the basic concept of attention is that each time the model predicts an output word, it only pays attention to some input words. Specifically, for each word in the output sentence, it will map the important and relevant words from the input sentence and assign higher weights to these words as shown in the figure below:</p>
<div class="inline-figure"><img src="Figures/transformers_01.png" alt="image-20220816201407808" style="zoom: 67%;"></div>
<p>In the traditional Seq2Seq model, we discard all the intermediate states of the encoder and use only its final states (vector) to initialize the decoder. The central idea behind attention is not to throw away these intermediate encoder states but to utilize all the states.</p>
</div>
<div id="steps-for-computing-attention" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Steps for Computing Attention<a class="anchor" aria-label="anchor" href="#steps-for-computing-attention"><i class="fas fa-link"></i></a>
</h3>
<p>The general attention mechanism makes use of three main components: queries (Q), keys (K), and values (V). It performs the following computations.</p>
<ol style="list-style-type: decimal">
<li>For each query vector <span class="math inline">\(q\)</span>, it is matched against a database of keys to compute a score value for each key. In other words, for each key <span class="math inline">\(k_i\)</span>, the dot product is computed: <span class="math inline">\(e_{q,k_i} = q\cdot k_i\)</span>.</li>
<li>The scores, which are computed for all keys, are then passed through a softmax operation to generate the weights: <span class="math inline">\(\alpha_{q,k_i} = softmax(e_{q,k_i})\)</span>.</li>
<li>The value vector <span class="math inline">\(v_{k_i}\)</span> , which is paired with a corresponding key, is weighted by the previously computed weights: <span class="math inline">\(\sum_{i} \alpha_{q,k_i} v_{k_i}\)</span>.</li>
</ol>
<p>In the context of machine translation, each word in an input sentence would be attributed its own query, key and value vectors. There are three different weight matrices that generate these three vectors. There are also different types of attention mechanisms, which are detailed at <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">this blog</a>.</p>
</div>
<div id="self-attention" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Self-Attention<a class="anchor" aria-label="anchor" href="#self-attention"><i class="fas fa-link"></i></a>
</h3>
<p>Self-attention is equivalent to a generalized attention mechanism where the query, key, and values take the same input. This is the attention mechanism used in the transformer architecture. It is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence.</p>
<p>For clarity, suppose the input sequence is “Today is Monday.”</p>
<ol style="list-style-type: decimal">
<li>For each token in the input sequence, compute the <code>query</code>, <code>key</code>, and <code>value</code> vectors using the three weight matrices. If the embedding vector size is 5 and the vector size of Q, K, and V is 3, then the weight matrices have the dimension <span class="math inline">\(5\times 3\)</span>.</li>
<li>Compute the attention score using the <code>query</code> and <code>key</code> vector by computing the dot product. For example, if the <code>query</code> word is “Today”, then one computes the dot product with key vectors for “Today”, “Is”, and “Monday.” This yields a vector of three numbers, since there are three tokens in the input sequence.</li>
<li>Using the softmax() function, compute the weights. This still has a vector of three numbers that now sum up to 1.</li>
<li>Multiply the weights to the <code>value</code> vector. Since the <code>value</code> vector has dimension <span class="math inline">\(1\times3\)</span>, the resulting output also has the same dimension.</li>
</ol>
<p>In sum, when the sequence “Today is Monday” is inputted into the self-attention layer as three <span class="math inline">\(5\times1\)</span> vectors, the output is three <span class="math inline">\(3\times1\)</span> vectors. In short notation, this is often expressed as:
<span class="math display">\[
Attention(Q,K,V) = Softmax(QK^\top)V
\]</span>
which summarizes the above steps in a simple form.</p>
<p><strong>Scaled Dot-Product Attention</strong></p>
<p>The Transformer implements a scaled dot-product attention. As the name suggests, it first computes a dot product for each query with all of the keys. It subsequently divides each result by <span class="math inline">\(\sqrt{d_k}\)</span> where <span class="math inline">\(d_k\)</span> is the dimension of the key vector.</p>
<p><strong>Multi-Head Attention</strong></p>
<p>Multi-head attention simply means that the self-attention is applied repeatedly. Specifically, we not only have one but multiple sets of Query/Key/Value weight matrices, and each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>
</div>
</div>
<div id="transformers" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Transformers<a class="anchor" aria-label="anchor" href="#transformers"><i class="fas fa-link"></i></a>
</h2>
<div id="overview-1" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Overview<a class="anchor" aria-label="anchor" href="#overview-1"><i class="fas fa-link"></i></a>
</h3>
<p>In its simplest form, a transformer takes in a sentence to translate and outputs a translated sentence:
<span class="math display">\[
input \to transformer \to output
\]</span>
The transformer consists of an encoder and a decoder:
<span class="math display">\[
input \to encoder \to decoder \to output
\]</span>
The encoder and the decoder consist of layers:</p>
<div class="inline-figure"><img src="Figures/transformers_02.png" alt="image-20220816204337867" style="zoom:50%;"></div>
<ul>
<li>Notice that the sublayers are stacked linearly for both the encoder and the decoder, but the decoder takes an additional input: the output from the last encoder sub-layer.</li>
</ul>
<p>Finally, each layer of the encoder and decoder take the following structure:</p>
<div class="inline-figure"><img src="Figures/transformers_03.png" alt="image-20220816204337867" style="zoom:50%;"></div>
<ul>
<li>The encoder layer computes the self-attention and then feeds it into the feed-forward network.</li>
<li>The decoder layer is identical to the encoder layer with additional middle step: the encoder-decoder attention, which uses the information carried over from the last step of the encoder.</li>
</ul>
</div>
<div id="step-1.-embedding" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> Step 1. Embedding<a class="anchor" aria-label="anchor" href="#step-1.-embedding"><i class="fas fa-link"></i></a>
</h3>
<p>Before the input sentence can be fed into the architecture, it needs to be embedded via (1) word embedding and (2) positional encoding. Obviously, this embedding only happens in the bottom-most encoder. Typically, the word embedding is done through a pre-trained neural network instead of a simple one-hot encoding.</p>
<p><strong>Positional encoding</strong> is also necessary because the transformer architecture does not include a default method for analyzing the order of words in the input sentence. In transformers, each position (index) is mapped to a vector, and hence the output of the positional encoding layer is a matrix where the <span class="math inline">\(i\)</span>th row is a vector representing the <span class="math inline">\(i\)</span>th token in the input sequence. This vector is then added to the original word embedding and fed into the very first encoder layer. (See <a href="https://jalammar.github.io/illustrated-transformer/">this post</a> for details on the construction of this matrix.)</p>
</div>
<div id="step-2.-self-attention-in-encoders" class="section level3" number="8.2.3">
<h3>
<span class="header-section-number">8.2.3</span> Step 2. Self-Attention in Encoders<a class="anchor" aria-label="anchor" href="#step-2.-self-attention-in-encoders"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Self-attention</strong> is a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. For example, suppose we are given the following input sentence: “Simon called Manav because he felt overwhelmed.” Self-attention allows the model to associate “he” with “Simon” and not “Manav.”</p>
<p>For each word embedding vector, there is a vector of the attention layer. But we require the entire input sequence (i.e. the list of words) for computing self-attention because (1) we need it to compute the weights (= normalized scores) and (2) we need to combine the weights with the value vectors, each of which is associated with each word input. <a href="https://jalammar.github.io/illustrated-transformer/">This post</a> describes the step-by-step process for computing self-attention very nicely, which is summarized below:</p>
<div class="inline-figure"><img src="Figures/transformers_04.png" alt="image-20220816214440095" style="zoom:50%;"></div>
<p><strong>Step 1. Create Query, Key, and Value vectors</strong></p>
<p>The first step is to create, for each word, a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that trained during the training process. Note that typically these new vectors are smaller in dimension than the embedding vector.</p>
<p><strong>Step 2. Compute the Score</strong></p>
<p>For each word, the score is is calculated by taking the dot product of the query vector with the key vector of the respective word. So for the word in position <span class="math inline">\(i\)</span>, the first score would be the dot product of <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_1\)</span> and the second score would be the dot product of <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_2\)</span>.</p>
<p><strong>Step 3. Convert the Scores to Weights</strong></p>
<p>The scores are divided by <span class="math inline">\(\sqrt{d}\)</span> where <span class="math inline">\(d\)</span> is the dimension of the key vectors, which is intended to lead to more stable gradients. Then they are passed through a softmax operation so that the scores are converted to weights. The resulting softmax score determines how much each word will be expressed at this position.</p>
<p><strong>Step 4. Construct the self attention layer</strong></p>
<p>Each value vector is multiplied by the softmax score and then the weighted value vectors are summed to produce the output of the self-attention layer.</p>
</div>
<div id="step-3.-self-attention-in-decoders" class="section level3" number="8.2.4">
<h3>
<span class="header-section-number">8.2.4</span> Step 3. Self-Attention in Decoders<a class="anchor" aria-label="anchor" href="#step-3.-self-attention-in-decoders"><i class="fas fa-link"></i></a>
</h3>
<p>Similarly as before, the decoder passes its input into a multi-head self-attention layer. Unlike the one in the encoder, it is only allowed to attend to earlier positions in the sequence. This is done by masking future positions.</p>
</div>
<div id="step-4.-encoder-decoder-attention-in-decoders" class="section level3" number="8.2.5">
<h3>
<span class="header-section-number">8.2.5</span> Step 4. Encoder-Decoder Attention in Decoders<a class="anchor" aria-label="anchor" href="#step-4.-encoder-decoder-attention-in-decoders"><i class="fas fa-link"></i></a>
</h3>
<p>This additional layer works like self-attention except that it combines two sources of inputs: the self-attention layer below it as well as the output of the encoder stack. <strong>Importantly, the output from the encoder stack is passed to the value and key parameters, while the output of the self-attention module is passed to the query parameter.</strong></p>
</div>
</div>
<div id="references" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> References<a class="anchor" aria-label="anchor" href="#references"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>
<a href="https://www.apronus.com/math/transformer-language-model-definition">This post</a> describes the mathematics behind the attention mechanism.</li>
<li>
<a href="https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/McmHduRWJynsjZjx5">This post</a> describes the Transformer architecture in a simplified form, and <a href="https://jalammar.github.io/illustrated-transformer/">this post</a> provides a nice illustration of how transformers work.</li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sequence-models-rnns-and-lstms.html"><span class="header-section-number">7</span> Sequence Models: RNNs and LSTMs</a></div>
<div class="next"><a href="pretrained-models-and-fine-tuning.html"><span class="header-section-number">9</span> Pretrained Models and Fine-Tuning</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#attention-models-and-transformers"><span class="header-section-number">8</span> Attention Models and Transformers</a></li>
<li>
<a class="nav-link" href="#attention"><span class="header-section-number">8.1</span> Attention</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-idea"><span class="header-section-number">8.1.1</span> Basic Idea</a></li>
<li><a class="nav-link" href="#steps-for-computing-attention"><span class="header-section-number">8.1.2</span> Steps for Computing Attention</a></li>
<li><a class="nav-link" href="#self-attention"><span class="header-section-number">8.1.3</span> Self-Attention</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#transformers"><span class="header-section-number">8.2</span> Transformers</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#overview-1"><span class="header-section-number">8.2.1</span> Overview</a></li>
<li><a class="nav-link" href="#step-1.-embedding"><span class="header-section-number">8.2.2</span> Step 1. Embedding</a></li>
<li><a class="nav-link" href="#step-2.-self-attention-in-encoders"><span class="header-section-number">8.2.3</span> Step 2. Self-Attention in Encoders</a></li>
<li><a class="nav-link" href="#step-3.-self-attention-in-decoders"><span class="header-section-number">8.2.4</span> Step 3. Self-Attention in Decoders</a></li>
<li><a class="nav-link" href="#step-4.-encoder-decoder-attention-in-decoders"><span class="header-section-number">8.2.5</span> Step 4. Encoder-Decoder Attention in Decoders</a></li>
</ul>
</li>
<li><a class="nav-link" href="#references"><span class="header-section-number">8.3</span> References</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>NLP Galore</strong>" was written by Various Sources. It was last built on 2022-09-06.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
