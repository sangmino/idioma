[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"handbook natural language processing internal use .Please cite.","code":""},{"path":"dictionary-based-methods.html","id":"dictionary-based-methods","chapter":"2 Dictionary-Based Methods","heading":"2 Dictionary-Based Methods","text":"","code":""},{"path":"regression-based-methods.html","id":"regression-based-methods","chapter":"3 Regression-Based Methods","heading":"3 Regression-Based Methods","text":"","code":""},{"path":"bag-of-words-tf-idf.html","id":"bag-of-words-tf-idf","chapter":"4 Bag of Words: TF-IDF","heading":"4 Bag of Words: TF-IDF","text":"","code":""},{"path":"bag-of-words-tf-idf.html","id":"word-tokenization","chapter":"4 Bag of Words: TF-IDF","heading":"4.1 Word Tokenization","text":"Tokenizers can easily become complex.","code":""},{"path":"bag-of-words-tf-idf.html","id":"implementing-tokenization","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1 Implementing Tokenization","text":"Several Python libraries implement tokenizers, advantages disadvantages:spaCy—Accurate , flexible, fast, PythonspaCy—Accurate , flexible, fast, PythonStanford CoreNLP—accurate, less flexible, fast, depends Java 8Stanford CoreNLP—accurate, less flexible, fast, depends Java 8NLTK—Standard used many NLP contests comparisons, popular, PythonNLTK—Standard used many NLP contests comparisons, popular, PythonNLTK Stanford CoreNLP around longest widely used comparison NLP algorithms academic papers.","code":""},{"path":"bag-of-words-tf-idf.html","id":"n-grams","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1.1 N-Grams","text":"n-gram sequence containing n elements extracted sequence elements, usually string.sequence tokens vectorized bag--words vector, loses lot meaning inherent order words. extending concept token include multiword tokens, n-grams, NLP pipeline can retain much meaning inherent order words statements.","code":""},{"path":"bag-of-words-tf-idf.html","id":"techniques-for-normalizing-the-vocabulary","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1.2 Techniques for Normalizing the Vocabulary","text":"One can normalize vocabulary tokens mean similar things combined single, normalized form. reduces number tokens need retain vocabulary also improves association meaning across different “spellings” token n-gram corpusCase FoldingCase folding consolidate multiple “spellings” word differ capitalization. can normalize capitalization using list comprehension: [x.lower() x tokens]StemmingAnother common vocabulary normalization technique eliminate small mean- ing differences pluralization possessive endings words, even various verb forms. Stemming removes suffixes words attempt combine words similar meanings together common stem. stem isn’t required properly spelled word, merely token, label, representing several possible spellings word.’s important note stemming greatly reduce “precision” score search engine,\nmight return many irrelevant documents along relevant ones.Two popular stemming algorithms Porter Snowball stemmers. Porter stemmer named computer scientist Martin Porter. Porter also responsible enhancing Porter stemmer create Snowball stemmer.LemmatizationIf access information connections meanings various words, might able associate several words together even spelling quite different. extensive normalization semantic root word—lemma—called lemmatization.Lemmatization potentially accurate way normalize word stemming case normalization takes account word’s meaning. lemmatizer uses knowledge base word synonyms word endings ensure words mean similar things consolidated single token.lemmatizers better stemmers applications. Stemmers really used large-scale information retrieval applications (keyword search).","code":"from nltk.stem.porter\nimport PorterStemmer\nstemmer = PorterStemmer()\n' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()]))nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize(\"better\", pos=\"a\") # if POS is not specified, it assumes noun"},{"path":"bag-of-words-tf-idf.html","id":"bag-of-words","chapter":"4 Bag of Words: TF-IDF","heading":"4.2 Bag of Words","text":"bag words representation text describes occurrence words within document.just keep track word counts disregard grammatical details word order. called “bag” words information order structure words document discarded. model concerned whether known words occur document, document.","code":""},{"path":"bag-of-words-tf-idf.html","id":"n-gram-adaptation","chapter":"4 Bag of Words: TF-IDF","heading":"4.2.1 N-Gram Adaptation","text":"general, bigrams make tokens understandable. Suppose two sentences:Sentence 1: “good job. miss anything”Sentence 2: ”good ”let us take vocabulary 5 words : “good,” “job,” “miss,” “,” “.”case, respective vectors sentences :Sentence 1: “good job. miss anything”=[1,1,1,1,0]Sentence 2: ”good ”=[1,0,0,1,1]Sentence 2 negative sentence, yet distinction reflected vectors.Now, suppose instead use bigrams, case Sentence 2 can broken : “,” “,” “good,” “good ,” “.” case, model can differentiate sentence 1 sentence 2.","code":""},{"path":"bag-of-words-tf-idf.html","id":"tf-idf","chapter":"4 Bag of Words: TF-IDF","heading":"4.3 TF-IDF","text":"TF-IDF intended reflect relevant term given document. computed multiplying two different metrics:Term Frequency (TF) word \\(t\\) document \\(d\\) given \n\\[\ntf(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\d} f_{t',d}}\n\\]\n\\(f_{t,d}\\) raw count term document. denominator total number terms document \\(d\\).Term Frequency (TF) word \\(t\\) document \\(d\\) given \n\\[\ntf(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\d} f_{t',d}}\n\\]\n\\(f_{t,d}\\) raw count term document. denominator total number terms document \\(d\\).Inverse Document Frequency (IDF) word \\(t\\) set documents \\(D\\) given \n\\[\nidf(t,D) = \\log\\frac{N}{|\\{d\\D:t\\d\\}|}\n\\]\n\\(N=|D|\\) denominator number documents term \\(t\\) appears.Inverse Document Frequency (IDF) word \\(t\\) set documents \\(D\\) given \n\\[\nidf(t,D) = \\log\\frac{N}{|\\{d\\D:t\\d\\}|}\n\\]\n\\(N=|D|\\) denominator number documents term \\(t\\) appears.TF-IDF calculated :\n\\[\ntfidf(t,d,D) = tf(t,d)\\times idf(t,D)\n\\]\nhigh weight TF-IDF reached high term frequency (given document) low document frequency term whole collection documents","code":""},{"path":"bag-of-words-tf-idf.html","id":"limitations-of-bag-of-words","chapter":"4 Bag of Words: TF-IDF","heading":"4.4 Limitations of Bag of Words","text":"Although Bag--Words quite efficient easy implement, still disadvantages.First, model ignores location information word. location information piece important information text. example “today ” “today ,” exact vector representation BoW model.Second, model ignores semantics word. example, words ‘soccer’ ‘football’ often used context. However, vectors corresponding words quite different bag words model.Third, range vocabulary big issue faced Bag--Words model. model comes across new word, ends ignoring word.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"basic-word-embeddings-word2vec-glove","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5 Basic Word Embeddings: Word2Vec & Glove","text":"","code":""},{"path":"topic-modelling-lsa-lda.html","id":"topic-modelling-lsa-lda","chapter":"6 Topic Modelling: LSA & LDA","heading":"6 Topic Modelling: LSA & LDA","text":"","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"sequence-models-rnns-and-lstms","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7 Sequence Models: RNNs and LSTMs","text":"","code":""},{"path":"attention-models-and-transformers.html","id":"attention-models-and-transformers","chapter":"8 Attention Models and Transformers","heading":"8 Attention Models and Transformers","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"other-useful-methods-for-textual-analysis","chapter":"9 Other Useful Methods for Textual Analysis","heading":"9 Other Useful Methods for Textual Analysis","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"convolutional-neural-networks-cnns","chapter":"9 Other Useful Methods for Textual Analysis","heading":"9.1 Convolutional Neural Networks (CNNs)","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"hidden-markov-models-hmms","chapter":"9 Other Useful Methods for Textual Analysis","heading":"9.2 Hidden Markov Models (HMMs)","text":"","code":""},{"path":"sentiment-analysis.html","id":"sentiment-analysis","chapter":"10 Sentiment Analysis","heading":"10 Sentiment Analysis","text":"","code":""},{"path":"data-scraping.html","id":"data-scraping","chapter":"11 Data Scraping","heading":"11 Data Scraping","text":"","code":""},{"path":"data-cleaning.html","id":"data-cleaning","chapter":"12 Data Cleaning","heading":"12 Data Cleaning","text":"","code":""}]
