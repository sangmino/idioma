[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"handbook natural language processing internal use .Please cite.","code":""},{"path":"dictionary-based-methods.html","id":"dictionary-based-methods","chapter":"2 Dictionary-Based Methods","heading":"2 Dictionary-Based Methods","text":"first describe dictionary-based methods, far common least resource-intensive way conducting textual analysis.","code":""},{"path":"dictionary-based-methods.html","id":"a-stylized-guide","chapter":"2 Dictionary-Based Methods","heading":"2.1 A Stylized Guide","text":"Many papers employing dictionary-based methods follow cookbook:want measure, important?measure want measure? corpus use? dictionary use?economic variables interested explaining measurement?Many well-known papers follow format:Cieslak Vissing-Jorgensen (2021) study Fed’s attention stock market, important given prominent role conduct monetary policy. measure object, use FOMC documents corpus utilize self-created dictionary stock market keywords. relate measured attention Fed’s interest rate decisions updates growth expectations.Sautner, Lent, Vilkov, Zhang (2022) interested measuring firm-level exposure climate change. Given rising climate risk accompanying regulator attention, measuring exposures key importance. measure object, use earnings conference calls corpus utilize dictionary created using keyword-discovery algorithm. relate measured exposures green technology hiring patenting well prices options equity claims.Hassan et al. (2022) seeks measure country risk perceived investors firms, critical determinant capital movements across countries. use earnings conference calls corpus use synonyms risk uncertainty obtained Oxford dictionary, library tone words Loughran McDonald (2011), self-created dictionary country-specific keywords. Ultimately, relate measured country risk capital flows sudden stops, firm-level investment employment decisions, fluctuations exchange rates.","code":""},{"path":"dictionary-based-methods.html","id":"often-used-dictionaries","chapter":"2 Dictionary-Based Methods","heading":"2.2 Often-Used Dictionaries","text":"","code":""},{"path":"dictionary-based-methods.html","id":"loughran-mcdonald-lm-dictionary","chapter":"2 Dictionary-Based Methods","heading":"2.2.1 Loughran-McDonald (LM) Dictionary","text":"Loughran McDonald (2011) created six different word lists (negative, positive, uncertainty, litigious, strong modal, weak modal) examining word usage large sample 10-Ks period 1994–2008. dictionary become predominant many studies, papers used LM word lists (primarily negative words) gauge tone business communication. Data can obtained link.","code":""},{"path":"regression-based-methods.html","id":"regression-based-methods","chapter":"3 Regression-Based Methods","heading":"3 Regression-Based Methods","text":"Using document characteristics, methods try predict word frequencies given document. natural way model relationships using multinomial logistic regression; however, due large dimensionality vocabularies, standard multinomial logistic regressions computationally feasible implement. recent years, econometric breakthroughs led development computationally feasible approximations ideal multinomial logit regression (Taddy, 2015).models several applications. Firstly, method tells us text depends observed covariates. relevant settings dependent variable text, e.g., political speeches. Secondly, estimated, model can inverted get predicted values covariates using text input. particularly powerful () nowcasting/forecasting (ii) backcasting data missing (Kelly et al., 2021). example, measurements real activity GDP available lag quarterly frequency. However, economic media coverage available real-time low latency. Hence, can used nowcast GDP. Similarly, data short histories, text can used backcast data.","code":""},{"path":"regression-based-methods.html","id":"challenges-of-having-text-as-the-dependent-variable","chapter":"3 Regression-Based Methods","heading":"3.1 Challenges of having text as the dependent variable","text":"typical unit analysis text data frequency n-gram given document, e.g., number times word “GDP” turned document. frequencies unordered uncategorical data, .e., obvious combine information document ten mentions word “GDP” six mentions word “inflation.” Hence, frequency counts represented document \\(\\) specific vectors sit \\(\\mathbf{c}_{}\\\\mathbb{R}^V\\) \\(V\\) number unique n-grams sample. Note \\(V\\) extremely large practice machine learning methods needed.go-model explain document \\(\\) specific count vector \\(\\mathbf{c}_{}\\) using variables \\(\\mathbf{v}_{}\\) multinomial logit regression,\n\\[\n\\mathrm{p}\\left(\\mathbf{c}_{} \\mid \\mathbf{v}_{}, m_{}\\right) = \\mathrm{MN}\\left(\\mathbf{c}_{} ; \\mathbf{q}_{}, m_{}\\right) \\text{ } =1 \\ldots n\n\\]\\[\nq_{j} = \\frac{e^{\\eta_{j}}}{\\sum_{k=1}^{d} e^{\\eta_{k}}} \\text { } j=1 \\ldots d\n\\]\\[\n\\eta_{j} = \\alpha_{j}+\\mathbf{v}_{}^{\\prime} \\varphi_{j}\n\\]\\(q_{j}\\) n-gram’s probability, \\(m_i\\) total number n-grams document, \\((\\alpha_j,\\varphi_j)\\) \\(K+1\\) n-gram specific parameters.Given vocabularies often exceed \\(10,000\\) n-grams, estimating model even just two covariates mean estimating \\(30,000+\\) Parameters! issue standard multinomial estimation parallelized; hence, estimating large number parameters computationally prohibitive.Multinomial logit regressions parallelized denominator \\(q_{ij}\\) depends parameters model. Hence, estimate model, need parallelizable approximation. Taddy (2015) provided precisely approximation used develop distributed multinomial regression (DMR)","code":""},{"path":"regression-based-methods.html","id":"distributed-multinomial-regressions-dmr","chapter":"3 Regression-Based Methods","heading":"3.2 Distributed multinomial regressions (DMR)","text":"canonical relationship Multinomial distributions can exactly decomposed independent Poissons,\n\\[\n\\operatorname{MN}\\left(\\mathbf{c}_{} ; \\mathbf{q}_{}, m_{}\\right)=\\frac{\\prod_{j} P_{o}\\left(c_{j} ; e^{\\eta_{j}}\\right)}{P o\\left(m_{} ; \\sum_{j=1}^{d} e^{\\eta_{j}}\\right)}\n\\]\ndecomposition solve computation bottleneck since denominator still depends parameters. However, Tadd (2015) used decomposition develop following approximation,\n\\[\n\\mathrm{p}\\left(\\mathbf{c}_{} \\mid \\mathbf{v}_{}, m_{}\\right)=\\mathrm{MN}\\left(\\mathbf{c}_{} ; \\mathbf{q}_{}, m_{}\\right) \\approx \\prod_{j} \\operatorname{Po}\\left(c_{j} ; m_{} e^{\\eta_{j}}\\right)\n\\]\nSince approximation doesn’t depend \\(\\sum_{j=1}^{d} e^{\\eta_{j}}\\)can parallelized! Essentially, n-gram’s parameters estimated separately using (shifted) Poisson regressions.Taddy (2015) also developed method inverting DMR post-estimation n-grams can projected covariates.","code":""},{"path":"regression-based-methods.html","id":"extension-hurdle-dmr-hdmr","chapter":"3 Regression-Based Methods","heading":"3.3 Extension: Hurdle DMR (HDMR)","text":"practice, DMR works well explaining strictly positive frequency counts (.e., counts > 0), performs poorly explaining whether word used (.e., count 0 1). Kelly et al. (2021) developed HDMR simultaneously explain intensive margin (strictly positive frequency counts) extensive margin (counts zero one) word choice.high level, model combines selection model DMR. selection model approximates extensive margin, DMR models intensive margin.Specifically, selection model include n-gram \\(j\\) document \\(\\) standard,\n\\[\nh_{j}^{*}=\\gamma_{}+\\kappa_{j}+\\boldsymbol{w}_{}^{\\prime} \\boldsymbol{\\delta}_{j}+v_{j}\n\\]\n\\[\nh_{j}=\\mathbf{1}\\left(h_{j}^{*}>0\\right)\n\\]\\(\\boldsymbol{w}_{}\\) observed variables \\((\\gamma_{},\\kappa_{j},\\boldsymbol{\\delta}_{j})\\) parameters. count model following form,\n\\[\nc_{j}^{*} =\\lambda\\left(\\mu_{}+\\alpha_{j}+\\mathbf{v}_{}^{\\prime} \\boldsymbol{\\varphi}_{j}\\right)+\\varepsilon_{j}\n\\]\n\\[\nc_{j} = \\left(1+c_{j}^{*}\\right) h_{j}\n\\]\\(\\boldsymbol{v}_{}\\) observed variables \\((\\mu_i,\\alpha_j,\\boldsymbol{\\varphi}_{j})\\) parameters. \\(\\lambda(\\cdot)\\geq 0\\), hence second equation restricts positive counts. Like DMR, method also parallelizable inversion method projecting text covariates.","code":""},{"path":"regression-based-methods.html","id":"application-1-using-text-to-measure-traditionally-difficult-to-quantify-concepts","chapter":"3 Regression-Based Methods","heading":"3.4 Application 1: using text to measure traditionally difficult to quantify concepts","text":"Gentzkow et al. (2019) used DMR measure partisanship US politicals—traditionally allusive concept quantify. defined partisanshp posterior probability, observer neutral prior poltician, expects identify speaker’s party hearing teh speaker utter single word. Given definition, use DMR estimate n-gram probabilities Democrats adn Republicans. map empirical distribution posterior belief observer neutral prior assigns speaker beign Republican utters prase \\(j\\) session \\(t\\) characteristics \\(x\\). definition partisanship considers extensive margin word choice e.g. whether poltican chooses use n-gram “pro-life” speaches .Kelly et al. (2021) expand definition partisanship include extensive intensive margin word choice .e. partisanshp posterior probability, observer neutral prior poltician, expects identify speaker’s party hearing speaker utter single word words speach. definition considers whether word “pro-choice” used, also often used. Since, Kelly et al. (2021) definition partisanship considers extensive intensive margin word choice use HDMR.figure shows resulting partisanship measures two measures. left hand side figure shows results using DMR extensive margin definition partisanship. DMR estimate suggests particanship significantly increased last 20 years. right hand side figure whows intenshive margin also considered, see spikes partisanship 20’s addition current period .","code":""},{"path":"regression-based-methods.html","id":"application-2-using-text-data-to-forecast-nowcast-and-backcast-hard-data","chapter":"3 Regression-Based Methods","heading":"3.5 Application 2: Using text data to forecast, nowcast and backcast hard data","text":"linear regression model \\(y_i = \\sum_{k} x_{,k}\\beta_k\\) estimated, can can use \\(y_i\\), \\(\\hat \\beta\\) \\(x_{,2},\\dots, x_{,K}\\) get fitted value \\(x_{,1}\\). Similarly DMR HDMR, model estimated, can use text data get fitted values covariates inverse regression. Hence, text data can used forecast, nowcast backcast hard data.example, text data can help better nowcasting forecasting macro variables. See Kelly et al. (2021) application. Text data typically available higher frequency less latency macro data, hence, can potentially improve macro variable nowcasts forecasts.Similarly, often many data series short-time series, whereas newspaper articles topic available much back. case news articles can used backcast data short time series. example, Kelly et al. (2021) demonstrate text data (along financial data) can used extend back intermediary capital ratio (ICR) back 1930s—ICR available post 70s period.","code":""},{"path":"bag-of-words-tf-idf.html","id":"bag-of-words-tf-idf","chapter":"4 Bag of Words: TF-IDF","heading":"4 Bag of Words: TF-IDF","text":"","code":""},{"path":"bag-of-words-tf-idf.html","id":"bag-of-words","chapter":"4 Bag of Words: TF-IDF","heading":"4.1 Bag of Words","text":"bag words representation text describes occurrence words within document. just keep track word counts disregard grammatical details word order. called “bag” words information order structure words document discarded. model concerned whether known words occur document, document.","code":""},{"path":"bag-of-words-tf-idf.html","id":"bag-of-n-grams","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1 Bag of N-Grams","text":"","code":""},{"path":"bag-of-words-tf-idf.html","id":"the-idea","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1.1 The Idea","text":"general, bigrams make tokens understandable. Suppose two sentences:Sentence 1: “good job. miss anything”Sentence 2: ”good ”let us take vocabulary 5 words : “good”, “job”, “miss”, “”, “.”case, respective vectors sentences :Sentence 1: “good job. miss anything” = [1,1,1,1,0]Sentence 2: ”good ” = [1,0,0,1,1]Sentence 2 negative sentence, yet distinction reflected vectors.Now, suppose instead use bigrams, case Sentence 2 can broken : “”, “”, “good”, “good ”, “.” case, model can differentiate sentence 1 sentence 2.","code":""},{"path":"bag-of-words-tf-idf.html","id":"pre-processing-to-reduce-feature-space","chapter":"4 Bag of Words: TF-IDF","heading":"4.1.1.2 Pre-Processing to Reduce Feature Space","text":"Using N-grams increases feature space, a1. techniques used reduce :Removing stopwords , , , , , etc.Stemming: process reducing word word stem. example, words like swimmer, swimming, swim, mapped one-word swim.Chunking Parts--speech tagging: One can use techniques find meaningful words sentence use feature vector","code":""},{"path":"bag-of-words-tf-idf.html","id":"tf-idf","chapter":"4 Bag of Words: TF-IDF","heading":"4.2 TF-IDF","text":"TF-IDF intended reflect relevant term given document. makes rare words prominent effectively ignores common words. Therefore, unlike bag--words, creates normalized count word count divided number documents word appears .computed multiplying two different metrics:Term Frequency (TF) word \\(t\\) document \\(d\\) given \n\\[\ntf(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\d} f_{t',d}}\n\\]\n\\(f_{t,d}\\) raw count term document. denominator total number terms document \\(d\\).Term Frequency (TF) word \\(t\\) document \\(d\\) given \n\\[\ntf(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\d} f_{t',d}}\n\\]\n\\(f_{t,d}\\) raw count term document. denominator total number terms document \\(d\\).Inverse Document Frequency (IDF) word \\(t\\) set documents \\(D\\) given \n\\[\nidf(t,D) = \\log\\frac{N}{|\\{d\\D:t\\d\\}|}\n\\]\n\\(N=|D|\\) denominator number documents term \\(t\\) appears.Inverse Document Frequency (IDF) word \\(t\\) set documents \\(D\\) given \n\\[\nidf(t,D) = \\log\\frac{N}{|\\{d\\D:t\\d\\}|}\n\\]\n\\(N=|D|\\) denominator number documents term \\(t\\) appears.TF-IDF calculated :\n\\[\ntfidf(t,d,D) = tf(t,d)\\times idf(t,D)\n\\]\nhigh weight TF-IDF reached high term frequency (given document) low document frequency term whole collection documents","code":""},{"path":"bag-of-words-tf-idf.html","id":"limitations-of-bag-of-words","chapter":"4 Bag of Words: TF-IDF","heading":"4.3 Limitations of Bag of Words","text":"Although Bag--Words quite efficient easy implement, still disadvantages.suffers curse dimensionality total dimension generally vocabulary size. Therefore, vocabulary needs designed carefully manage size. time, bag words also often leads sparse vectors, require memory computational resources.suffers curse dimensionality total dimension generally vocabulary size. Therefore, vocabulary needs designed carefully manage size. time, bag words also often leads sparse vectors, require memory computational resources.model ignores location information word. location information piece important information text. example “today ” “today ”, exact vector representation BoW model.model ignores location information word. location information piece important information text. example “today ” “today ”, exact vector representation BoW model.model ignores semantics word. example, words ‘soccer’ ‘football’ often used context. However, vectors corresponding words quite different bag words model.model ignores semantics word. example, words ‘soccer’ ‘football’ often used context. However, vectors corresponding words quite different bag words model.range vocabulary big issue faced Bag--Words model. model comes across new word, ends ignoring word.range vocabulary big issue faced Bag--Words model. model comes across new word, ends ignoring word.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"basic-word-embeddings-word2vec-glove","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5 Basic Word Embeddings: Word2Vec & Glove","text":"","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"overview","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.1 Overview","text":"Word embedding simply refers representing words word vectors. idea large corpus text every word fixed vocabulary represented vector.Need Word EmbeddingVarious word encoding methods like Integer/ Label Encoding One-Hot Encoding many limitations. main limitation encoding doesn’t semantic relationship words. Therefore, alternative approach learn encode similarity vectors .addition, one-hot encoding, memory requirement features space increasing vocabulary size.Generating Word EmbeddingsThere generally two methods generating word embeddings: (1) SVD based methods (2) neural network (iteration) based methods.SVD based embedding methods, first, create matrix co-occurrence reduce dimensionality matrix using SVD. applying SVD, word vocabulary embedding reduced space.\nmethods effectively leverage global statistical information, primarily used capture word similarities poorly tasks word analogy, indicating sub-optimal vector space structure.SVD based embedding methods, first, create matrix co-occurrence reduce dimensionality matrix using SVD. applying SVD, word vocabulary embedding reduced space.methods effectively leverage global statistical information, primarily used capture word similarities poorly tasks word analogy, indicating sub-optimal vector space structure.neural net based methods, instead computing storing global information huge dataset, one can try create model able learn one iteration time.\nmethods shallow window-based, learn word embeddings making predictions local context windows.neural net based methods, instead computing storing global information huge dataset, one can try create model able learn one iteration time.methods shallow window-based, learn word embeddings making predictions local context windows.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"svd-based-methods","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.2 SVD Based Methods","text":"One basic idea accumulate word co-occurrence counts matrix \\(X\\) perform Singular Value Decomposition (SVD) \\(X\\) get \\(USV^\\top\\) decomposition. first \\(k\\) columns \\(U\\) can used \\(k\\)-dimensional word vectors.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"limitations","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.2.1 Limitations","text":"numerous limitations SVD based methods:Generally, matrix high dimension, results high training cost \\(O(mn^2)\\) \\(m\\) size dictionary \\(n\\) embedding size. corpus large number words, becomes difficult train.matrix quickly becomes imbalanced due high frequency words.dimension matrix changes soon new word introduced.get around problem, one can use following tricks:Ignore common words like “”, “” etc.weight two different words considered based distance two words, rather just raw count.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"iteration-based-methods","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.3 Iteration Based Methods","text":"idea design model whose parameters word vectors.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"word2vec","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.3.1 Word2Vec","text":"Word2Vec one popular technique learn word embeddings using shallow neural network, developed Tomas Mikolov 2013 Google. shallow, two-layer neural network trained reconstruct linguistic contexts words.takes large corpus words input outputs vector space hundreds dimensions, unique word corpus allocated corresponding vector space.can obtained using two model architectures: Continuous Bag Words (CBOW) Skip-gram. CBOW aims predict center word surrounding context terms word vectors. Skip-gram opposite, predicts distribution (probability) context words center word.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"method-1-cbow","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.3.1.1 Method #1: CBOW","text":"core idea predict center word surrounding context. word \\(w_i\\), learn 2 vectors: \\(v_i\\), representation word context, \\(u_i\\), representation word center.Using Model goal learn two matrices, \\(\\mathcal{V}\\\\mathbb{R}^{n\\times|V|}\\) \\(\\mathcal{U}\\\\mathbb{R}^{|V|\\times n}\\). \\(\\mathcal{V}\\) input word matrix \\(\\)th column \\(v_i\\), \\(\\mathcal{U}\\) output word matrix \\(j\\)th row \\(u_j\\). Equipped matrix, model predicts center word following steps:input context size \\(m\\), one hot word vectors generated: \\(x^{(c-m)}, ..., x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\\\\mathbb{R}^{|V|}\\)obtain embeddings vector via \\(v = \\mathcal{V}x\\).average vectors get \\(\\hat{v}\\) generate score vector \\(z=\\mathcal{U}\\hat{v}\\\\mathbb{R}^{|V|}\\).Turn scores probabilities via \\(\\hat{y} = softmax(z) \\\\mathbb{R}^{|V|}\\), ’d like close true probability \\(y\\) – happens one hot vector actual word – much possible.Training ModelIn training model, often use cross entropy \\(H(\\hat{y}, y)\\) measure distance:\n\\[\nH(\\hat{y}, y) = - \\sum_{j=1}^{|V|} y_j \\log (\\hat{y}_j)\n\\]\nTherefore, optimization problem can framed minimizing objective \\(J\\) \n\\[\nJ = -\\log P(w_c|w_{c-m}, ..., w_{c-1}, w_{c+1}, ..., w_{c+m})\n\\]\nUsing notations embeddings,\n\\[\n=-\\log P(u_c|\\hat{v}) = -\\log \\frac{\\exp(u_c^\\top \\hat{v})}{\\sum_{j=1}^{|V|} \\exp(u_j^\\top \\hat{v})}\n\\]\nTherefore can use stochastic gradient descent update relevant word vectors \\(u_c\\) \\(v_j\\).","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"method-2-skip-gram","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.3.1.2 Method #2: Skip-Gram","text":"core idea predict surrounding context words given center word. setup largely CBOW role center context words reversed.Using Model Generate one hot input vector \\(x\\\\mathbb{R}^{|V|}\\) center word.Use embeddings get embedded word vector center word: \\(v_c = \\mathcal{V}x\\\\mathbb{R}^n\\).Generate score vector \\(z=\\mathcal{U}v_c\\).Turn score vector probabilities \\(\\hat{y} = softmax(z)\\) yields probabilities observing context word: \\(\\hat{y}_{c-m}, ..., \\hat{y}_{c-1}, \\hat{y}_{c+1}, ..., \\hat{y}_{c+m}\\). , want probability vectors match one hot vectors actual output.Training ModelGiven task bit daunting, need one additional assumption strong conditional independence. words, given center word, output words completely independent.Therefore, objective can written :\n\\[\nJ = -\\log P(w_{c-m}, ..., w_{c+m}|w_c) = -\\log \\prod_{j=0, j\\neq m}^{2m} P(w_{c-m+j} | w_c)\n\\]\n, applying embeddings yields:\n\\[\n=-\\log \\prod_{j=0, j\\neq m}^{2m} P(u_{c-m+j} | v_c) = -\\log \\prod_{j=0,j\\neq m}^{2m} \\frac{\\exp(u_{c-m+j}^\\top v_c)}{\\sum_{k=1}^{|V|}\\exp(u_{k}^\\top v_c)}\n\\]\nessence, Skip-gram treats context word equally: model computes probability word appearing context independently distance center word.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"implementation-details","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.3.1.3 Implementation Details","text":"According original paper, found Skip-Gram works well small datasets, can better represent less frequent words. However, CBOW found train faster Skip-Gram, can better represent frequent words.original authors method also provide two implementation details improve training performance: () negative sampling (ii) hierarchical softmax.Negative SamplingBy defining new objective function, negative sampling aims maximizing similarity words context minimizing occur different contexts. However, instead minimization words dictionary except context words, randomly selects handful words depending training size uses optimize objective.Heirarchical SoftmaxHeirarchical softmax replacement softmax much faster evaluate: softmax \\(O(n)\\) time, hierarchical softmax \\(O(\\log n)\\) time.can view softmax function tree, root node hidden layer activations (context vector C ), leaves probabilities word. hierarchical softmax instead computes values leaves multi-layer tree. evaluate probability given word, take product probabilities edge path node.","code":""},{"path":"basic-word-embeddings-word2vec-glove.html","id":"best-of-both-worlds-glove","chapter":"5 Basic Word Embeddings: Word2Vec & Glove","heading":"5.4 Best of Both Worlds: GloVe","text":"starting point GloVe reconcile linear algebra algorithms neural updating algorithms. main advantage GloVe unlike Word2vec, GloVe just rely local statistics also incorporates global statistics (word co-occurrence) obtain word vectors.main insight ratio co-occurrence probabilities two words (rather co-occurrence probabilities ) contains information look encode information vector differences.ModelLet \\(X\\) denote co-occurrence matrix \\(X_{ij}\\) number times word \\(j\\) occurs context word \\(\\). denote \\(P_{ij} = P(w_j | w_i) = X_{ij} / X_i\\) probability \\(j\\) appearing context word \\(\\) \\(X_i = \\sum_{k} X_{ik}\\).one can see, populating \\(X\\) computing \\(P\\) requires single pass entire corpus, one-time -front cost.objective \\(J\\) global cross-entropy loss:\n\\[\nJ = - \\sum_{\\corpus} \\sum_{j\\context()} \\log Q_{ij}\n\\]\n\\(Q_{ij}\\) probability word \\(j\\) appearing context word \\(\\) skip-gram model. can rewrite objective \n\\[\nJ= - \\sum_{=1}^W \\sum_{j=1}^W X_{ij} \\log Q_{ij}\n\\]\nSince \\(Q_{ij}\\) requires normalization thus incurs summation entire vocabulary, least squares objective used instead:\n\\[\n\\hat{J} = \\sum_{=1}^W \\sum_{j=1}^W X_i (\\hat{P}_{ij} - \\hat{Q}_{ij})^2\n\\]\n\\(\\hat{P}_{ij} = X_{ij}\\) \\(\\hat{Q}_{ij} = \\exp(u_j^\\top v_i)\\) unnormalized distribution. since \\(X_{ij}\\) can large, instead minimize squared error log space introduce general weighting scheme:\n\\[\n\\hat{J} = \\sum_{=1}^W \\sum_{j=1}^W f(X_{ij}) (u_j^\\top v_i - \\log X_{ij})^2\n\\]","code":""},{"path":"topic-modelling-lda.html","id":"topic-modelling-lda","chapter":"6 Topic Modelling: LDA","heading":"6 Topic Modelling: LDA","text":"goal topic modeling extract latent topics observed documents. example, may collection news articles given article may discuss economy, geopolitics, ; goal topic modeling extract topics unsupervised manner. Alternatively one can also think topic modeling dimension reduction technique; documents high-dimensional objects containing large collection words, like map lower-dimensional topic space.popular topic model Latent Dirichlet Allocation (LDA) first proposed Blei et al. (2003). LDA unsupervised technique, takes input corpus documents outputs latent topics .e. factors load words. chapter, cover LDAs.","code":""},{"path":"topic-modelling-lda.html","id":"notation-and-terminology","chapter":"6 Topic Modelling: LDA","heading":"6.1 Notation and Terminology","text":"Words indexed \\(1,\\dots,V\\) represented basis vector—collection words make corpus referred vocabulary.Topics denoted \\(z_1,\\dots,z_K\\). topic models discussed , researcher need specify number \\(K\\) latent topics trying extract.Document \\(m\\) represented sequence sequence \\(N\\) words, \\(d_m = \\{w_1,w_2,\\dots,w_N\\}\\) e.g. newspaper article. simplicity, unless otherwise stated, throughout chapter going assume length documents exactly \\(N\\) words.Corpus composed collection \\(M\\) documents \\(\\mathcal{D}=\\{d_1,\\dots, d_M\\}\\) e.g. New York Times.Document-term matrix way describe often words occur across documents corpus \\(X\\\\mathbb{R}^{M\\times V}\\).\nbasic version involve simply frequency count possible \\(V\\) words \\(M\\) documents.\nbasic version involve simply frequency count possible \\(V\\) words \\(M\\) documents.","code":""},{"path":"topic-modelling-lda.html","id":"models-that-preceded-the-lda","chapter":"6 Topic Modelling: LDA","heading":"6.2 Models that preceded the LDA:","text":"diving LDAs, useful cover couple models preceded LDA. serves three pedagogical purposes. Firstly, helps see deep parallels topic modeling techniques commonly used dimension reduction methods principle component analysis (PCA). Secondly, motivates exact challenges faced past methods LDAs trying address, also shows challenges still remain.","code":""},{"path":"topic-modelling-lda.html","id":"latent-semantic-models-lsa","chapter":"6 Topic Modelling: LDA","heading":"6.2.1 Latent Semantic Models (LSA)","text":"One way think topic modeling trying get low-dimensional representation documents, low-dimensional representations can thought topics. Hence, one might attempt simply apply PCA document-term matrix, interpret principle components topics. precisely idea behind LSAs.LSA applies Singular Value Decomposition (SVD) document-term matrix interprets extracted factors topics. method excellent dimension reduction, allow interpretability, since PCA identifies loadings orthogonal rotations. Additionally, SVDs require large amount data get reasonable estimates. reasons, researchers decided put parametric structure problem.","code":""},{"path":"topic-modelling-lda.html","id":"probabilistic-latent-semantic-models-plsa","chapter":"6 Topic Modelling: LDA","heading":"6.2.2 Probabilistic Latent Semantic Models (pLSA)","text":"next iteration LSA put structure problem using two-level hierarchical Bayesian set-model data generating process document. DGP provides joint distribution documents, words, topics. try estimate posterior latent topics conditional observed documents words. can use map documents topics.parameterization direct parallels SVD (see great explanation ), allows greater interpretability efficient estimation. worth discussing specifics pLSA, LDA directly builds .pLSA’s DGP attempts capture two key properties intuitively think documents, topics words related: () document can composed multiple topics (e.g. article can economics geopolitics), (ii) topics tend associated set words (e.g. economics associated words like GDP, inflation, currency, etc.). pLSA captures former modeling document multinomial distribution topics drawn—-document-specific multinomial parameter captures document-specific topic distribution. Similarly, captures latter modeling words drawn topic-specific multinomial distributions. formally, DGP given document \\(m\\) containing \\(N\\) words follows,\\(N\\) words \\(w_n\\) document \\(m\\):\nChoose topic \\(z_{n} \\sim Multinomial(\\theta_m)\\).\nChoose word \\(w_n\\) multinomial \\(p(w_n|z_n;\\beta)\\) \\(\\beta\\) \\(V\\times K\\) matrix column topic-specific word distribution.\nChoose topic \\(z_{n} \\sim Multinomial(\\theta_m)\\).Choose word \\(w_n\\) multinomial \\(p(w_n|z_n;\\beta)\\) \\(\\beta\\) \\(V\\times K\\) matrix column topic-specific word distribution.pLSA algorithm can represented using flow chart (augmented Blei et al. (2003)) can graphically represented follows,pLSA’s major improvement LSA. Namely, assuming latent topics drawn multinomial distributions, able uniquely identify topic loadings allow greater interpretability. However, major limitation pLSA requires estimating multinomial parameter \\(\\theta_m\\) document separately. result, pLSA requires document large number words estimate parameters consistently. Hence, practice, pLSA can quite unstable. LDA’s built top pLSAs solve issue.","code":""},{"path":"topic-modelling-lda.html","id":"latent-dirichlet-allocation-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3 Latent Dirichlet Allocation (LDA)","text":"efficiently use data across documents, LDAs assume document-specific distribution drawn Dirichlet distribution \\(\\theta_m \\sim Dir(\\alpha)\\). Bayesian hierarchical structure helps parameter shrinkage; \\(\\theta_m\\) documents fewer words shrunk towards population mean. makes model stable pLSA.LDA algorithm follows,document \\(m\\) choose \\(\\theta_m \\sim Dir(\\alpha)\\)document \\(m\\) choose \\(\\theta_m \\sim Dir(\\alpha)\\)\\(N\\) words \\(w_n\\) document \\(m\\):\nChoose topic \\(z_{n} \\sim Multinomial(\\theta)\\).\nChoose word \\(w_n\\) multinomial \\(p(w_n|z_n;\\beta)\\) \\(\\beta\\) \\(V\\times K\\) matrix column topic-specific word distribution.\n\\(N\\) words \\(w_n\\) document \\(m\\):Choose topic \\(z_{n} \\sim Multinomial(\\theta)\\).Choose topic \\(z_{n} \\sim Multinomial(\\theta)\\).Choose word \\(w_n\\) multinomial \\(p(w_n|z_n;\\beta)\\) \\(\\beta\\) \\(V\\times K\\) matrix column topic-specific word distribution.Choose word \\(w_n\\) multinomial \\(p(w_n|z_n;\\beta)\\) \\(\\beta\\) \\(V\\times K\\) matrix column topic-specific word distribution.Similarly, can graphically represented follows (Blei et al. (2003)).Hopefully, clear algorithm figures, main difference LDAs pLSAs additional assumption \\(\\theta_m\\) drawn Dirichlet distribution.","code":""},{"path":"topic-modelling-lda.html","id":"challenges-of-working-with-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3.1 Challenges of working with LDA","text":"Note Dirchilet prior helps shrinkage topic-document distribution, still left issue number parameters grows vocabulary growth— β V × K α V × 1. Hence, make LDA’s work practice considerable pre-processing data needed ensure V isn’t big model can estimated easily.recently, methods combining word embeddings LDAs proposed solve challenge.","code":""},{"path":"topic-modelling-lda.html","id":"intractable-likelihood-and-estimation","chapter":"6 Topic Modelling: LDA","heading":"6.3.2 Intractable likelihood and estimation","text":"probability observing given words terms model parameters ,\\[p(\\mathbf{w} \\mid \\alpha, \\beta)=\\frac{\\Gamma\\left(\\sum_{} \\alpha_{}\\right)}{\\prod_{} \\Gamma\\left(\\alpha_{}\\right)} \\int\\left(\\prod_{=1}^{k} \\theta_{}^{\\alpha_{}-1}\\right)\\left(\\prod_{n=1}^{N} \\sum_{=1}^{k} \\prod_{j=1}^{V}\\left(\\theta_{} \\beta_{j}\\right)^{w_{n}^{j}}\\right) d \\theta\\]can see quite messy, makes intractable find analytical expression model parameters \\(\\alpha\\) \\(\\beta\\). Hence, need rely numerical methods MCMC variational inference (VI). standard many ML settings, due large dimensionality problem, preferred method estimating LDAs VI.","code":""},{"path":"topic-modelling-lda.html","id":"packages-for-implementing-the-standard-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3.3 Packages for implementing the standard LDA","text":"Python packages can implement LDAs box,sklearn.decomposition.LatentDirichletAllocationgensim.models.ldamodel.LdaModel","code":""},{"path":"topic-modelling-lda.html","id":"extensions-to-the-standard-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3.4 Extensions to the standard LDA","text":"","code":""},{"path":"topic-modelling-lda.html","id":"dynamic-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3.4.1 Dynamic LDA","text":"LDA assumes parameters \\(\\alpha\\) \\(\\beta\\) constant time. However, may appropriate assumption longer horizons. example, topics discussed scientific journals considerably changed last century. naive method addressing issue repeatedly estimate LDA year time. However, approach throws lot useful information, topics given year somewhat informative topics following year. Hence, dynamic LDA developed Blei Lafferty (2006).dynamics LDA attempts estimate sequence parameters \\((\\alpha_t,\\beta_t)\\). essentially connects many LDA’s together assuming \\(\\alpha_t\\) \\(\\beta_t\\) evolve random walked Gaussian shocks (can extracted using Kalman filters). algorithm ,Draw topics \\(\\beta_{t} \\mid \\beta_{t-1} \\sim \\mathcal{N}\\left(\\beta_{t-1}, \\sigma^{2} \\right)\\).Draw \\(\\alpha_{t} \\mid \\alpha_{t-1} \\sim \\mathcal{N}\\left(\\alpha_{t-1}, \\delta^{2} \\right)\\).document:Draw \\(\\eta \\sim \\mathcal{N}\\left(\\alpha_{t}, ^{2} \\right)\\)word:Draw \\(Z \\sim \\operatorname{Mult}(\\pi(\\eta))\\).Draw \\(W_{t, d, n} \\sim \\operatorname{Mult}\\left(\\pi\\left(\\beta_{t, z}\\right)\\right)\\).\\(\\pi\\) maps multinomial natural parameters mean parameters, \\(\\pi\\left(\\beta_{k, t}\\right)_{w}=\\frac{\\exp \\left(\\beta_{k, t, w}\\right)}{\\sum_{w} \\exp \\left(\\beta_{k, t, w}\\right)}\\).Graphically can represented ,","code":""},{"path":"topic-modelling-lda.html","id":"supervised-lda","chapter":"6 Topic Modelling: LDA","heading":"6.3.4.2 Supervised LDA","text":"contexts, text data also response variable. example, movie ratings, can see people’s reviews rating give movie. use paired data help better identify topics (also predict responses using text data). idea behind supervised LDA (sLDA) (Blei McAuliffe, 2007).sLDA word function latent topic (like standard LDA), also document-specific response variable function latent topics given document. DGP described algorithm,Draw topic proportions \\(\\theta \\mid \\alpha \\sim \\operatorname{Dir}(\\alpha)\\).word\nDraw topic assignment \\(z_{n} \\mid \\theta \\sim \\operatorname{Mult}(\\theta)\\).\nDraw word \\(w_{n} \\mid z_{n}, \\beta_{1: K} \\sim \\operatorname{Mult}\\left(\\beta_{z_{n}}\\right)\\).\nDraw topic assignment \\(z_{n} \\mid \\theta \\sim \\operatorname{Mult}(\\theta)\\).Draw word \\(w_{n} \\mid z_{n}, \\beta_{1: K} \\sim \\operatorname{Mult}\\left(\\beta_{z_{n}}\\right)\\).Draw response variable \\(y \\mid z_{1: N}, \\eta, \\sigma^{2} \\sim \\mathrm{N}\\left(\\eta^{\\top} \\bar{z}, \\sigma^{2}\\right)\\).\\(\\bar z\\) mean topics document.Graphically can represented ,","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"sequence-models-rnns-and-lstms","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7 Sequence Models: RNNs and LSTMs","text":"","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"sequence-models","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.1 Sequence Models","text":"Unlike models work inputs consisting single feature vector \\(\\mathbf{x}\\\\mathbb{R}^d\\), sequence models work inputs consist ordered list feature vectors \\(\\mathbf{x}_1, ..., \\mathbf{x}_m\\) feature vector now indexed time sequence step \\(t\\).context natural language processing, often talk language models. Specifically, goal language models compute probability sequence \\(m\\) words, usually conditioned window \\(n\\) previous words, opposed previous words:\n\\[\nP(w_1, ..., w_m) = \\prod_{=1}^{=m} P(w_i | w_1,...,w_{-1}) \\approx \\prod_{=1}^{=m} P(w_i | w_{-n},...,w_{-1})\n\\]","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"n-gram-language-models","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.1.1 n-gram Language Models","text":"One way compute probabilities compare frequency word frequency \\(n\\)-gram contains word.Bigram language model:\n\\[\nP(w_2|w_1) = \\frac{count(w_1, w_2)}{count(w_1)}\n\\]Bigram language model:\n\\[\nP(w_2|w_1) = \\frac{count(w_1, w_2)}{count(w_1)}\n\\]Trigram language model:\n\\[\nP(w_3|w_1, w_2) = \\frac{count(w_1, w_2, w_3)}{count(w_1, w_2)}\n\\]Trigram language model:\n\\[\nP(w_3|w_1, w_2) = \\frac{count(w_1, w_2, w_3)}{count(w_1, w_2)}\n\\]approach runs obvious issues. First, numerator may zero due sparsity. One solution add small \\(\\delta\\) count word vocabulary. Second, denominator may zero due sparsity, case may “back ’” condition \\(count(w_2)\\) rather \\(count(w_1, w_2)\\). Third, since step requires storing \\(n\\)-grams, may issues storage.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"rnn-neural-networks-with-memory","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.2 RNN: Neural Networks with Memory","text":"Recurrent Neural Networks (RNNs) family neural networks capture dynamics sequences via recurrent connections. idea instead burdening model predicting output one go, allow much easier task predicting iterative sub-outputs, sub-output improvement refinement previous step.Hidden State = MemoryThe core idea instead modeling \\(P(w_m | w_{m-1}, ..., w_{m-n+1})\\) instead preferable use latent variable model:\n\\[\nP(w_t | w_{t-1}, ..., w_1)=P(w_t | h_{t-1})\n\\]\n\\(h_{t-1}\\) hidden state stores sequence information time step \\(t-1\\). general, hidden state can depend current input previous hidden state: \\(h_t = f(x_t, h_{t-1})\\).RecurrenceWe can also view RNNs traditional neural networks enhanced loop, one allows information persist across timesteps – hence name, “recurrent.” [also implies weight matrix applied.]","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"basic-rnn-architecture","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.2.1 Basic RNN Architecture","text":"RNN can represented internal hidden state \\(h_t\\) output \\(o_t\\):\\[\n\\begin{cases}\nh_{t} & =f\\left(W_{xh}x_{t}+W_{hh}h_{t-1}+b_{h}\\right)\\\\\no_{t} & =g\\left(W_{hy}h_{t}+b_{o}\\right)\n\\end{cases}\n\\]\\(W_{xh}x_t\\) : Input \\(x_t\\) multiplied weight matrix \\(W_{xh}\\) extract information input\\(W_{hh}h_{t-1}\\): Previous hidden state \\(h_{t-1}\\) multiplied weight matrix \\(W_{hh}\\) extract information “memory”Activation functions: \\(f\\) usually tanh ReLU g often softmax\\(b_h\\) \\(b_o\\) biases.AdvantagesAs continue emphasize, recurrent neural networks can model sequence data – either time-series words sentence – sample assumed dependent previous ones. modelling perspective, also convenience since can process input sequences length, model size increase longer input sequence lengths. possible computation step \\(t\\) can use information many steps back.DisadvantagesOne notable disadvantage training RNNs slow since sequential parallelized. practice, difficult access information many steps back due problems like vanishing exploding gradients, discuss later section.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"training-rnns","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.2.2 Training RNNs","text":"Backpropagation Time (BPTT)Backpropagation time (BPTT) simply backpropagation applied sequence models hidden state thus used train recurrent neural networks. relegate details implementation .","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"vanishing-and-exploding-gradients","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.2.3 Vanishing and Exploding Gradients","text":"Vanishing GradientsIn backpropagation, gradients frequently become smaller new model weights end virtually identical old weights without updates. result, gradient descent algorithm never converges optimal solution. known problem vanishing gradients.happen? Vanishing gradients issue typically occur using sigmoid tanh activation functions hidden layer, effectively compresses large input space small output space. inputs become fairly small fairly large, derivatives become extremely close zero gradient value propagate back.One obvious solution replace activation function network using ReLU instead sigmoid. keeps linearity regions sigmoid tanh saturated, thus responding better gradient vanishing. Another solution consider different architecture LSTM, discuss later . post discusses potential changes.Exploding GradientsIf gradients get larger backpropagation progresses, may end outsized weight updates, thereby leading divergence gradient descent algorithm.happen? problem happens weights activation function. weight values large, derivatives also higher, thereby changing weights significantly preventing gradient converging.common solution “gradient clipping” one may simply clip parameter gradient element-wise parameter update clip norm gradient parameter update.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"applications","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.2.4 Applications","text":"Part--speech taggingThe process classifying words parts speech labeling accordingly known part--speech tagging, simply POS-tagging. post provides implementation using RNN.Text GenerationText generation one obvious application RNN architecture. training neural network predict next character, called Character Level Model. Similarly, can train model predict next word, given sequence words called Word Level Models.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"lstm","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.3 LSTM","text":"Long Short-Term Memory RNNs (LSTMs) type RNN proposed 1997 solution vanishing gradients problem. foreshadowed, major strength capturing long-term dependencies sequence. post provides detailed explanation understanding LSTM networks. provide essential details.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"basic-lstm-architecture","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.3.1 Basic LSTM Architecture","text":"Recall RNNs form chain repeating modules neural network. standard RNNs, repeating module simple structure, single tanh layer. LSTMs also chain like structure, repeating module different structure. Instead single neural network layer, four, interacting special way.LSTM addresses issues RNN maintaining cell state \\((c_t)\\), state given time. cell state updated time step, output hidden state derived input \\((x_t)\\), previous hidden state (\\(h_{t-1}\\)), updated cell state \\((c_t)\\).read, erase, write cell, also three corresponding gates. First forget gate, first orange box left. takes previous hidden state, input learned weights produce number 0 1. second input gate, consists next two orange boxes diagram. first sigmoid layer decides values update, next tanh layer creates vector candidate values can added states.Whether update indeed happens determined last output gate, consists last two orange boxes. first sigmoid layer decides parts cell state ’re going output, cell state put tanh layer multiplied output sigmoid gate, output parts decided . Note cell state also needs updated \\(c_{t-1}\\) \\(c_t\\). done horizontal arrow top diagram.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"variations","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.3.2 Variations","text":"One popular LSTM variant, introduced Gers & Schmidhuber (2000), adding “peephole connections.” means let gate layers look cell state. Another variant Gated Recurrent Unit (GRU), combines forget input gates single “update gate.” also merges cell state hidden state, makes changes. resulting model simpler standard LSTM models, growing increasingly popular. GRU exposes complete memory unlike LSTM simpler, easier modify faster train.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"other-extensions","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.4 Other Extensions","text":"","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"bidirectional-rnns","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.4.1 Bidirectional RNNs","text":"possible make predictions based future words RNN model read corpus backwards. bi-directional RNN therefore maintains two hidden layers, one left--right propagation another right--left propagation.Since bidirectional RNNs require access entire input sequence, applicable language modeling left context available. BERT one system built bidirectionality.","code":""},{"path":"sequence-models-rnns-and-lstms.html","id":"multi-layer-rnns","chapter":"7 Sequence Models: RNNs and LSTMs","heading":"7.4.2 Multi-layer RNNs","text":"One can stack RNNs construct multi-layer RNNs. system, hidden states RNN layer \\(\\) inputs RNN layer \\(+1\\). allows network compute complex representations.High-performing RNNs often multi-layer, ranging 2 4 layers. actually deep convolutional feed-forward networks. Transformer-based networks BERT usually deeper like 12 24 layers.","code":""},{"path":"attention-models-and-transformers.html","id":"attention-models-and-transformers","chapter":"8 Attention Models and Transformers","heading":"8 Attention Models and Transformers","text":"https://www.apronus.com/math/transformer-language-model-definition","code":""},{"path":"attention-models-and-transformers.html","id":"attention","chapter":"8 Attention Models and Transformers","heading":"8.1 Attention","text":"","code":""},{"path":"attention-models-and-transformers.html","id":"basic-idea","chapter":"8 Attention Models and Transformers","heading":"8.1.1 Basic Idea","text":"context language translation, basic concept attention time model predicts output word, pays attention input words. Specifically, word output sentence, map important relevant words input sentence assign higher weights words shown figure :image-20220816201407808In traditional Seq2Seq model, discard intermediate states encoder use final states (vector) initialize decoder. central idea behind attention throw away intermediate encoder states utilize states.","code":""},{"path":"attention-models-and-transformers.html","id":"steps-for-computing-attention","chapter":"8 Attention Models and Transformers","heading":"8.1.2 Steps for Computing Attention","text":"","code":""},{"path":"attention-models-and-transformers.html","id":"family-of-attention-mechanisms","chapter":"8 Attention Models and Transformers","heading":"8.1.3 Family of Attention Mechanisms","text":"different types attention mechanisms: https://lilianweng.github.io/posts/2018-06-24-attention/","code":""},{"path":"attention-models-and-transformers.html","id":"self-attention","chapter":"8 Attention Models and Transformers","heading":"8.1.4 Self-Attention","text":"Self-attention equivalent generalized attention mechanism query, key, values take input.","code":""},{"path":"attention-models-and-transformers.html","id":"transformers","chapter":"8 Attention Models and Transformers","heading":"8.2 Transformers","text":"https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/McmHduRWJynsjZjx5https://jalammar.github.io/illustrated-transformer/","code":""},{"path":"attention-models-and-transformers.html","id":"overview-1","chapter":"8 Attention Models and Transformers","heading":"8.2.1 Overview","text":"simplest form, transformer takes sentence translate outputs translated sentence:\n\\[\ninput \\transformer \\output\n\\]\ntransformer consists encoder decoder:\n\\[\ninput \\encoder \\decoder \\output\n\\]\nencoder decoder consist layers:image-20220816203617350Notice sublayers stacked linearly encoder decoder, decoder takes additional input: output last encoder sub-layer.Finally, layer encoder decoder take following structure:image-20220816204337867The encoder layer computes self-attention feeds feed-forward network.decoder layer identical encoder layer additional middle step: encoder-decoder attention, uses information carried last step encoder.","code":""},{"path":"attention-models-and-transformers.html","id":"step-1.-embedding","chapter":"8 Attention Models and Transformers","heading":"8.2.2 Step 1. Embedding","text":"input sentence can fed architecture, needs embedded via (1) word embedding (2) positional encoding. Obviously, embedding happens bottom-encoder.word embedding typically done pre-trained neural network instead simple one-hot encoding.Positional encoding also necessary transformer architecture include default method analyzing order words input sentence. transformers, position (index) mapped vector, hence output positional encoding layer matrix \\(\\)th row vector representing \\(\\)th token input sequence. vector added original word embedding fed first encoder layer.See post details construction matrix.","code":""},{"path":"attention-models-and-transformers.html","id":"step-2.-self-attention-in-encoders","chapter":"8 Attention Models and Transformers","heading":"8.2.3 Step 2. Self-Attention in Encoders","text":"Self-attention layer helps encoder look words input sentence encodes specific word. example, suppose given following input sentence: “Simon called Manav felt overwhelmed.” Self-attention allows model associate “” “Simon” “Manav.”word embedding vector, vector attention layer. require entire input sequence (.e. list words) computing self-attention (1) need compute weights (= normalized scores) (2) need combine weights value vectors, associated word input. post describes step--step process computing self-attention nicely, summarized :image-20220816214440095Step 1. Create Query, Key, Value vectorsThe first step create, word, Query vector, Key vector, Value vector. vectors created multiplying embedding three matrices trained training process.Note typically new vectors smaller dimension embedding vector.Step 2. Compute ScoreFor word, score calculated taking dot product query vector key vector respective word. word position \\(\\), first score dot product \\(q_i\\) \\(k_1\\) second score dot product \\(q_i\\) \\(k_2\\).Step 3. Convert Scores WeightsThe scores divided \\(\\sqrt{d}\\) \\(d\\) dimension key vectors, intended lead stable gradients. passed softmax operation scores converted weights. resulting softmax score determines much word expressed position.Step 4. Construct self attention layerEach value vector multiplied softmax score weighted value vectors summed produce output self-attention layer.Note: Multi-headed Attention","code":""},{"path":"attention-models-and-transformers.html","id":"step-3.-self-attention-in-decoders","chapter":"8 Attention Models and Transformers","heading":"8.2.4 Step 3. Self-Attention in Decoders","text":"Similarly , decoder passes input multi-head self-attention layer. Unlike one encoder, allowed attend earlier positions sequence. done masking future positions.","code":""},{"path":"attention-models-and-transformers.html","id":"step-4.-encoder-decoder-attention-in-decoders","chapter":"8 Attention Models and Transformers","heading":"8.2.5 Step 4. Encoder-Decoder Attention in Decoders","text":"additional layer works like self-attention except combines two sources inputs: self-attention layer well output encoder stack. Importantly, output encoder stack passed value key parameters, output self-attention module passed query parameter.","code":""},{"path":"pretrained-models-and-fine-tuning.html","id":"pretrained-models-and-fine-tuning","chapter":"9 Pretrained Models and Fine-Tuning","heading":"9 Pretrained Models and Fine-Tuning","text":"section, provide ver","code":""},{"path":"pretrained-models-and-fine-tuning.html","id":"bert","chapter":"9 Pretrained Models and Fine-Tuning","heading":"9.1 BERT","text":"","code":""},{"path":"pretrained-models-and-fine-tuning.html","id":"gpt","chapter":"9 Pretrained Models and Fine-Tuning","heading":"9.2 GPT","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"other-useful-methods-for-textual-analysis","chapter":"10 Other Useful Methods for Textual Analysis","heading":"10 Other Useful Methods for Textual Analysis","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"convolutional-neural-networks-cnns","chapter":"10 Other Useful Methods for Textual Analysis","heading":"10.1 Convolutional Neural Networks (CNNs)","text":"","code":""},{"path":"other-useful-methods-for-textual-analysis.html","id":"hidden-markov-models-hmms","chapter":"10 Other Useful Methods for Textual Analysis","heading":"10.2 Hidden Markov Models (HMMs)","text":"","code":""},{"path":"sentiment-analysis.html","id":"sentiment-analysis","chapter":"11 Sentiment Analysis","heading":"11 Sentiment Analysis","text":"","code":""},{"path":"data-scraping.html","id":"data-scraping","chapter":"12 Data Scraping","heading":"12 Data Scraping","text":"","code":""},{"path":"data-cleaning.html","id":"data-cleaning","chapter":"13 Data Cleaning","heading":"13 Data Cleaning","text":"","code":""},{"path":"data-cleaning.html","id":"word-tokenization","chapter":"13 Data Cleaning","heading":"13.1 Word Tokenization","text":"Tokenizers can easily become complex.","code":""},{"path":"data-cleaning.html","id":"implementing-tokenization","chapter":"13 Data Cleaning","heading":"13.1.1 Implementing Tokenization","text":"Several Python libraries implement tokenizers, advantages disadvantages:spaCy—Accurate , flexible, fast, PythonspaCy—Accurate , flexible, fast, PythonStanford CoreNLP—accurate, less flexible, fast, depends Java 8Stanford CoreNLP—accurate, less flexible, fast, depends Java 8NLTK—Standard used many NLP contests comparisons, popular, PythonNLTK—Standard used many NLP contests comparisons, popular, PythonNLTK Stanford CoreNLP around longest widely used comparison NLP algorithms academic papers.","code":""},{"path":"data-cleaning.html","id":"n-grams","chapter":"13 Data Cleaning","heading":"13.1.1.1 N-Grams","text":"n-gram sequence containing n elements extracted sequence elements, usually string.sequence tokens vectorized bag--words vector, loses lot meaning inherent order words. extending concept token include multiword tokens, n-grams, NLP pipeline can retain much meaning inherent order words statements.","code":""},{"path":"data-cleaning.html","id":"techniques-for-normalizing-the-vocabulary","chapter":"13 Data Cleaning","heading":"13.1.1.2 Techniques for Normalizing the Vocabulary","text":"One can normalize vocabulary tokens mean similar things combined single, normalized form. reduces number tokens need retain vocabulary also improves association meaning across different “spellings” token n-gram corpusCase FoldingCase folding consolidate multiple “spellings” word differ capitalization. can normalize capitalization using list comprehension: [x.lower() x tokens]StemmingAnother common vocabulary normalization technique eliminate small mean- ing differences pluralization possessive endings words, even various verb forms. Stemming removes suffixes words attempt combine words similar meanings together common stem. stem isn’t required properly spelled word, merely token, label, representing several possible spellings word.’s important note stemming greatly reduce “precision” score search engine,\nmight return many irrelevant documents along relevant ones.Two popular stemming algorithms Porter Snowball stemmers. Porter stemmer named computer scientist Martin Porter. Porter also responsible enhancing Porter stemmer create Snowball stemmer.LemmatizationIf access information connections meanings various words, might able associate several words together even spelling quite different. extensive normalization semantic root word—lemma—called lemmatization.Lemmatization potentially accurate way normalize word stemming case normalization takes account word’s meaning. lemmatizer uses knowledge base word synonyms word endings ensure words mean similar things consolidated single token.lemmatizers better stemmers applications. Stemmers really used large-scale information retrieval applications (keyword search).","code":"from nltk.stem.porter\nimport PorterStemmer\nstemmer = PorterStemmer()\n' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()]))nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize(\"better\", pos=\"a\") # if POS is not specified, it assumes noun"}]
